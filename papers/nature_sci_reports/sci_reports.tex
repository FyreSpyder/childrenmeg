\documentclass[fleqn,10pt]{wlscirep}
\usepackage{multirow, subcaption, amsmath, array}
\usepackage{color,soul}

\DeclareMathOperator*{\argmaxA}{arg\,max}

\title{Machine learning for MEG during speech tasks}

\author[1,2,*]{Demetres Kostas}
\author[1,3,4]{Elizabeth W. Pang}
\author[1,2,5]{Frank Rudzicz}
\affil[1]{University of Toronto; Toronto, Canada}
\affil[2]{Vector Institute; Toronto, Canada}
\affil[3]{Hospital for Sick Children; Toronto, Canada}
\affil[4]{SickKids Research Institutue; Toronto, Canada}
\affil[5]{Toronto Rehabilitation Institute-UHN; Toronto, Canada}

\affil[*]{demetres@cs.toronto.edu}

\begin{abstract}
  We consider whether a deep neural network trained with raw MEG data can be used to predict the age of children performing a verb-generation task, a monosyllable speech-elicitation task, and a multi-syllabic speech-elicitation task. Furthermore, we argue that the network makes predictions on the grounds of differences in speech development. Previous work has explored taking `deep' neural networks (DNNs) designed for, or trained with, images to classify encephalographic recordings with some success, but this does little to acknowledge the structure of these data. Simple neural networks have been used extensively to classify data expressed as features, but require extensive feature engineering and pre-processing. We present novel DNNs trained using raw magnetoencephalography (MEG) and electroencephalography (EEG) recordings that mimic the feature-engineering pipeline. We highlight criteria the networks use, including relative weighting of channels and preferred spectro-temporal characteristics of re-weighted channels. Our data feature 92 subjects aged 4-18, recorded using a 151-channel MEG system. Our proposed model scores over 95\% mean cross-validation accuracy distinguishing above and below $10$ years of age in single trials of un-seen subjects, and can classify publicly available EEG with state-of-the-art accuracy.

\end{abstract}
\begin{document}

\flushbottom
\maketitle

\thispagestyle{empty}

\section*{Introduction}

Speech development, from infancy to adulthood, is a remarkable and uniquely human process. Language and speech in the adult brain spans a diverse set of regions and interconnections from brain stem to cerebral cortex \cite{GuentherBook, Tourville2011, Hillis}, but higher level language abilities are typically left-hemisphere lateralized for approximately 90\% of the general population, although this  can range from 70\% to 95\% in populations with distinct left and right handedness respectively \cite{GuentherBook, Kadis2011, Yu2014}. Children develop this lateralization over time and show more bi-lateral activity as an inverse function of age \cite{Kadis2011, Ressel2008}. A common experimental paradigm for demonstrating this is the verb-generation task, which requires a subject to produce a verb that is associated with an object/noun with which they have been provided (presented for example, using an image) \cite{Kadis2011}. Speech articulation divorced from language is much less lateralized in comparison to {\em higher level} language faculties regardless of age \cite{GuentherBook}, and in terms of cortical systems, heavily engages the Rolandic cortex to integrate somatosensory and motor control representations to command the speech articulators \cite{GuentherBook}. Although this process is less lateralized than higher level language function, non-word monosyllabic, and multisyllabic experiments still show some left-lateralization \cite{Ghosh2008a}. We consider a dataset made up of a combination of a verb-generation task, the monosyllabic utterance /{\em pah}/ and the multisyllable non-word /{\em pah tah kah}/ with overlapping sets of subjects recorded using magnetoencephalography (MEG), to demonstrate age-related language and speech development.

Machine learning (ML) has been used in brain computer interfaces (BCIs), including for silent speech (imagined or covert) \cite{Sereshkeh2017, Guimaraes2007, Zhao2015a}. Typically, data are collected and pre-processed using a variety of techniques, such as cropping, trial averaging, normalization, band-pass filtering, and spatial transforms such as principle component analysis (PCA), independent component analysis (ICA) and common spatial patterns (CSP) \cite{RezaeiTabar2016,Muller-Gerking1999}. The goal of the preprocessing stage is to overcome the low signal-to-noise ratios typical of these recordings. After these initial steps, features are calculated, often representing spectral characteristics in the canonical brain frequency bands ($\delta$, $\theta$, $\alpha$, etc.), and summary statistics. 

An advantage of this approach is that expert knowledge about the data can help circumvent relatively poor signal quality and encourage ML models to distinguish between classes using established correlates. However, this comes with potential bias of expert knowledge, and the underlying assumptions it may reinforce. In contrast, `deep' ML models can achieve many of the stages of the pipeline intrinsically, and consider potentially unknown correlates. We propose two new DNN architectures, the first labelled SCNN is a mainly convolutional neural network (CNN), and the second labelled Ra-SCNN is an augmented SCNN network that employs a recurrent layer and attention mechanism. They are both outlined in figure \ref{fig:archs}, and detailed below. Both networks are trained with raw single trials, that exploit the pipeline typically used for BCIs. Here they are trained to predict the age of normally developing (in terms of speech abilities) children aged 4-18, performing at least two of three tasks commonly used to evaluate speech and language development. They are evaluated with single trials of subjects unseen during training. This training and evaluation context is established to encourage the DNNs to make predictions on the grounds of detected differences in the development of speech and/or language.

\begin{figure}[ht]
  \centering\includegraphics[width=0.5\linewidth]{pipeline.pdf}
   \caption{Diagram of the different processing steps performed for each dataset to compare the feature-based and end-to-end approaches. MEG and audio recordings are used with the feature-based approach to benchmark the end-end-end models using raw MEG. The best end-to-end model is then interrogated to reveal how decisions are made. Each colour represents a dataset configuration that is produced.}
 \label{fig:proc_stages}
\end{figure}

We compare the performance of a wide-range of classifiers, including our proposed models, when trained with features extracted from the MEG recordings, and as reference the audio recordings, of each trial. We herein refer to this process as a {\em feature-based} approach. This is compared to our proposed DNNs trained with the same MEG recordings that have been minimally processed, and optionally augmented in number through cropping, which we call an {\em end-to-end} approach. The best performing {\em end-to-end} model is then thoroughly interrogated: we evaluate the learned weights' carry-over performance predicting which of the three speech tasks is being performed, we use activation maximization to examine the most successful network's inner workings, and finally, we demonstrate that our models are a suitable classifier for encephalographic data more generally, by comparing the performance of our DNN architectures against the state-of-the-art for a public dataset of EEG recordings used in BCI applications.

\begin{figure}[t]
  \centering\includegraphics[width=\linewidth]{architectures.png}
  \caption{The two general CNN architectures we propose. Processing of trials flows from bottom to top, where the grey arrays represent an incoming trial of $C$ channels and $T$ samples (where for our primary dataset: $C=151$ and $T=400$). \textbf{(a)} Spatial summary convolutional network (SCNN), the architecture is broken down into three stages. The {\em Spatial Summary} stage performs convolutions exclusively across channels at each timepoint, thus not impacting the number of samples (the second dimension $T$ remains unchanged). Likewise, the {\em Temporal Filtering} stage performs convolutions exclusively across the time dimension. The output stage flattens the resulting output of the previous stages and uses a feed-forward network for final classification. The colours are used to distinguish between sequences produced as a result of different learned convolution kernels at each stage. \textbf{(b)} The Ra-SCNN network, which extends the SCNN presented in \textbf{(a)}. The output of the {\em Temporal Filtering} stage in \textbf{(a)} is fed as input to an LSTM enhanced with attention weighted input layer. Finally the output of the LSTM uses a feed-forward network for final classification.}
  \label{fig:archs}
\end{figure}

\section*{Results}

\subsection*{Age classification}

\begin{table}[htp]
  \caption{Classification accuracy of $age >= 10$ versus $age < 10$. We compare standard feature classifiers of logistic regression, linear kernel support vector machine (Linear SVM), and a fully-connected feed-forward neural network (FFNN) to our proposed models trained end-to-end. The end-to-end models are also trained with an augmentation strategy that artificially increases the number of training points using cropped sub-sequences of trials.}
  \centering
  \begin{tabular}{l | l  l | c | c }
    \toprule
    \textbf{Technique} & \textbf{Dataset} & \textbf{Model} & \textbf{Mean \%} & \textbf{Dev. \%} \\
    \toprule
    \multirow{10}{*}{Feature-Based} & \multirow{5}{*}{Audio} & Logistic Regression      & 54.3 & 6.44  \\
                                    &                        & Linear SVM               & 73.9 & 3.45  \\
                                    &                        & FFNN                     & 71.7 & 2.44  \\
                                    &                        & SCNN                     & 65.9 & 9.35  \\
                                    &                        & Ra-SCNN                  & 67.5 & 13.9  \\
    \cline{2-5}
                                    & \multirow{5}{*}{MEG}   & Logistic Regression    & 63.5 & 5.64  \\
                                    &                        & Linear SVM             & 69.2 & 5.74  \\
                                    &                        & FFNN                   & 66.3 & 4.93  \\
                                    &                        & SCNN                   & 69.7 & 7.03  \\
                                    &                        & Ra-SCNN                & 72.3 & 1.74  \\    
    \midrule
    \multirow{4}{*}{End-to-End}     & \multirow{2}{*}{MEG}   & SCNN                   & 95.1 & 2.31  \\
                                    &                        & Ra-SCNN                & 92.6 & 5.79  \\     
    \cline{2-5}
                                    & \multirow{2}{*}{MEG (+crop. aug.)}  & SCNN      & 89.2 & 7.93  \\
                                    &                        & Ra-SCNN                & 93.4 & 0.93  \\         
    \bottomrule
  \end{tabular}
 %\caption{Classification accuracy when constrained to the binary classification problem of $age >= 10$ versus $age < 10$.}
  \label{tab:binary_results}
\end{table}

Table \ref{tab:binary_results} shows a convincing difference between feature-based and end-to-end approaches. The end-to-end approaches all score nearly 90\% or higher mean five-fold cross validation accuracy. Analysis with a Friedman test considering performance of all folds clearly indicates real differences with $\chi^2_{13}=52.789$ and $p<10^{-6}$. All end-to-end models significantly outperform logistic regression trained with both audio and MEG features ($p<0.05$ after single-step correction), and the SCNN trained with raw un-augmented data also significantly outperforms the FFNN trained with the reduced MEG features dataset. Although this is not a unanimous demonstration that the end-to-end models are superior, we see this as a fairly promising indication of performance. Of particular interest are the SCNN models trained without crop augmentation and the Ra-SCNN model with crop augmentation.

\subsection*{Finer-Grained Prediction}

To model the fundamentally continuous nature of the phenomena that these data are meant to represent, we also considered targeting seven age categories, each two years in length, rather than the binary targets presented above. As might be expected, these do not yield accuracies as high as those from the binary models. However,  they are promising as their performance does indicate that errors are dominated by confusion with nearby age categories, rather than randomly distributed.

\subsection*{Task Prediction}

With the performance above, it is clear that the SCNN and Ra-SCNN architectures show some promise compared to a more conventional feature-based pipeline; however, it remains to be seen how much of this performance is a result of detected differences in speech development. Here we consider if the most successful model's parameters (i.e. the {\em previously learned} weights from above) apply directly to another speech task independent of age. Starting with an un-trained (randomly initialized) SCNN network with the same configuration as the previous task, the network is trained to predict which of the three tasks is being performed. This is compared to a SCNN network that is already trained to predict age, specifically the SCNN network that achieves the highest single fold accuracy in the age classification task. These weights are fixed, so that the network is used as a fixed feature extractor, and the final layer is replaced with a simple SVM classifier that is trained to classify these fixed features.

\begin{table}[h]
 \caption{Accuracy predicting which of the three speech tasks is being performed, across all five folds, using the highest performing SCNN architecture from the binary classification task. \textit{Newly Trained} represents a network with weights that are randomly initialized, and trained end-to-end to predict speech task. \textit{Previously Trained} describes the performance of the same architecture, but with weights fixed to the values from the highest performing single fold in the binary classification task. Instead, the model's final output layer is replaced with a SVM with a linear kernel, which is then trained to classify which task is performed.}
 \centering
 \begin{tabular}{l | c | c | c | c | c | c | c}
   \toprule
   \textbf{Model Weights} & \textbf{Fold 1} & \textbf{Fold 2} & \textbf{Fold 3} & \textbf{Fold 4} & \textbf{Fold 5} & \textbf{Mean \%} & \textbf{Dev. \%}\\
   \toprule
                        Newly Trained           & 47.1 & 49.0 & 48.0 & 41.7 & 47.7 & 46.7 & 2.88 \\
   \midrule
                        Previously Trained      & 48.6 & 47.1 & 46.5 & 47.3 & 44.7 & 46.8 & 1.41 \\ 
   \bottomrule
 \end{tabular}
 \label{tab:task_results}
\end{table}

We see in table \ref{tab:task_results} that both of these networks comfortably exceed the level of chance, and the $38.13\%$ majority class threshold (as there is an imbalance of classes, which is addressed during training with a penalized loss). These accuracies are not as impressive as the age prediction task, but they do have performance that is remarkably similar to each other. A paired Wilcoxon test shows these distributions are not significantly different, with $V_{4, two-sided}=9.0$ and $p=0.8125$. Therefore, the weights (and thus features) learned to accurately perform the age classification task perform equivalently to newly trained weights when predicting a separate speech task that is independent of age. We see this as evidence that the features are in some way speech related, rather than expressing characteristics correlated only to age.

\subsection*{Model Activations}

Maximizing the input at some point in the network with respect to an output downstream in the network (both intermediate nodes and final outputs) can provide insight into the preferred characteristics of incoming data. This approach has been used previously to show the characteristics of individual layers, and preferred input for image classification networks \cite{Yosinski2015}. We employed artificial input maximization using the model with the highest single fold test accuracy to minimize creating artificial data that had characteristics that were not pertinent to classifying age. This resulted in using the SCNN model trained with the second fold of the raw MEG dataset. Figure \ref{fig:max_components} demonstrates how the 151 MEG channels (bilinear interpolation over expected channel locations on scalp image)  maximize three spatial filters at the final spatial summary layer. Correspondingly, figure \ref{fig:max_spectrograms} shows the spectral content of maximizing these same spatial features with respect to the two output classes, juxtaposed with high confidence real activations of again the same spatial features.

\begin{figure}[h!]
  \centering\includegraphics[width=\linewidth]{components.eps}
  \caption[textfind]{Three examples of input channel relative intensities, interpolated using expected channel locations, that maximized the output of the spatial stage for three different components (kernels) at this stage. Components (a) and (b) show a common pattern of spatial mixing, with a particularly strong relative intensity localized around channels near the inferior frontal or perhaps dorsolateral prefrontal regions. This region is the only somewhat consistently strongly weighted region that is not exagerated by the effect of extrapolating beyond the . Component (c) shows a very nearly inverse set of inputs compared to (a), in addition to what could be strong emphasis over the visual cortex (although edge effects of the interpolation make this hard to say). The plots are constructed using bilinear interpolation so that they are mapped across expected channel locations. These plots were made using the MNE python library (available at \url{https://www.martinos.org/mne/stable/index.html})}
  \label{fig:max_components}
\end{figure}

\newcolumntype{v}{>{\centering\arraybackslash} m{.13\linewidth} }
\newcolumntype{V}{>{\centering\arraybackslash} m{.18\linewidth} }

\begin{figure}[ht]
  \caption{Power spectral density spectrograms in dB scale for data that maximizes output classes. Plots on the left represent artificially generated data that maximally activate the classes $<10$ and $>=10$ years old with respect to component 4 (figure \ref{fig:max_components}a), component 28 (figure \ref{fig:max_components}b) and component 49 (figure \ref{fig:max_components}c). Each row of spectrograms corresponds to components 4, 28 and 49 respectively. Plots on the right show the two real datapoints that most significantly maximized the two classes for the components of their corresponding rows.}
  \centering\includegraphics[width=\linewidth]{spectrograms.jpg}
 \label{fig:max_spectrograms}
\end{figure}

Throughout the learned components, there is a common patch of relative intensity found in approximately the inferior frontal or perhaps dorsolateral prefrontal regions. All three components shown in figure \ref{fig:max_components} demonstrate this to some degree (although (c) is a negative complement, this means very little since the outgoing weights may all just reverse this sign again). The artificially generated datapoints show fairly uniform spectrograms across all components, with spectral density concentrated between approximately $0-14$ H{\em z} in the $<10$ years old class, and concentration $15-22$ H$z$ for $>=10$ years old. All of the artificial datapoints are very stationary, and show no particular emphasis around the event time $t=1.5s$. The $>=10$ artificial data spectrograms do however show more activity overall in the high $beta$, low $gamma$ rhythms and this is particularly evident in components 4 and 49.

The real data, unlike the artificial data, unsurprisingly are not as stationary, and do tend to show a marked change after the $t=1.5s$ mark. Again, much like the artificial data, the spectro-temporal differnces between the three components are minimal.

\subsection*{Baseline Recordings}

The homogeneity of the artificial signals above could be evidence that classification is relying on some non-event related signals; for example, some correlation between resting frequency and age. To consider if this is the case, we again explore the highest-accuracy fold SCNN model from above, and consider how it well it classifies baseline recordings for each test subject and each experiment task, consisting of the brief recording of subjects before the sequence of trials for each task. This is compared to the performance of subtracting a simple model of baseline activity, made by averaging the inital recordings across all tasks for each subject, from an accurate test trial for each subject and test condition.

\begin{table}[h]
 \caption{Number of correct versus incorrect predictions when considering baseline recordings of held-out test subjects. Compared to subtracting average baseline of each subject from correct, and highly confident test trials with corresponding subject and task.}
 \centering
 \begin{tabular}{l | c | c | c}
   \toprule
   \textbf{Predictions} &  \textbf{Initial Baseline Alone} & \textbf{Test Minus Average Baseline} \\
   \toprule
                        Correct           & 15 & 23  \\
                        Incorrect         & 11 &  3  \\ 
   \bottomrule
 \end{tabular}
 \label{tab:rest_performance3}
\end{table}

Table \ref{tab:rest_performance3} suggests that classifying non-task recordings results in dramatically lower performance than classifying real trials, and subtracting the mean baseline from test points does not nearly affect performance as much. Testing for independence of the {\em Initial Baseline} and {\em Test Minus Average Baseline} groups shows a significant difference between the groups with $\chi^2_{2}=6.256$ and $p<0.013$.

\subsection*{Obscuring Trials}

To more distinctly see what sections of a trial have the most impact on performance, figure \ref{fig:obsc_profile} plots the performance of the SCNN model as a function of how much of the trial is obscured by white noise. We plot the performance of two different techniques: when the obscured portion is centred around the moment the subjects were cued to perform their task, labelled {\em Obscure Event}, and obscuring the ends of the trial, labelled {\em Obscure Ends}. Obscuring the trial event immediately reduces the performance of the model and steadily underperforms the {\em Obscure Ends} case. For approximately 250ms, obscuring the ends of a trial has no effect on performance, indicating that this portion of the data is likely unnecessary. What is of particular interest besides the consistent difference in performance, is the {\em steady} decrease in performance both models demonstrate. The lack of any particularly sharp and distinct drop within the first half second makes it unlikely that some age correlated event-related field (ERF) is solely responsible for the performance we demonstrate, as most ERFs (or event-related potential in the case of EEG) are typically shorter than a half-second \cite{ElectricFieldsOfTheBrain}. If this was the main contributor to performance, once the entire ERF was fully obscured the performance should have immediately dropped, instead there is a fairly steady drop for the first two seconds. It is of course still possible that an ERF plays a role in classification decisions, but it appears unlikely that they are a major contributor.

Obscuring the event gives a fairly consistent decrease in performance for at least the first two seconds of obscured points. Least-squares linear regression of this segment finds a slope of $-0.147$ ($p \ll 10^{-10}$ and correlation $r=-0.986$). Likewise when obscuring the ends, there appears to be a similar trend in performance between approximately 1 second and 3 seconds. For this segment the slope using least-squares linear regression is $-0.173$ ($p \ll 10^{-10}$ and correlation $r=-0.987$). Testing for similarity between the two slopes finds that the slope of these lines are not in fact the same ($p \ll 10^{-10}$, $t_{1394, two-sided}=-17.97$), so the decrease in performance does not happen identically in these sections.

\begin{figure}[h!]
  \caption{Output intensity of model corresponding to the correct prediction, when obscured by white noise localized at time of trial prompt (event) compared to white noise localized at the ends of trials. Solid lines correspond to average model output for the best performing trial of each subject, trial and 10 randomly drawn different noise signals. Translucent fill around solid lines indicates a single standard deviation across noise. Trial lengths considered here are 3.5s, thus for each sample obscured before trial or beginning of trial corresponds to 6 obscured after or at end of trial for the \textit{Obscure Event} and \textit{Obscure Ends} conditions respectively.}
  \centering\includegraphics[width=0.5\linewidth]{obscuring_profile.pdf}
 \label{fig:obsc_profile}
\end{figure}

\subsection*{Secondary dataset classification}

\begin{table}[h]
 \caption{Classification accuracy of SCNN and Ra-SCNN models proposed in this work as compared to the convolutional neural network implementation of a filter bank common spatial pattern (FBCSP) classifier introduced by Schirrmeister {\em et. al.}\cite{Schirrmeister2017}.}
 \centering
 \begin{tabular}{l l | c | c}
   \toprule
   \textbf{Dataset} & \textbf{Model} & \textbf{Mean \%} & \textbf{Dev. \%} \\
   \toprule
   \multirow{3}{*}{BCI IV 2a.}
                        & FBCSP-CNN           & 70.9 & 12.6  \\
                        & SCNN                & 72.6 & 10.7  \\
                        & Ra-SCNN             & 73.3 & 13.6  \\ 
   \midrule
   \multirow{3}{*}{BCI IV 2a. (+cropping aug.)}
                        & FBCSP-CNN           & 65.6 & 12.2  \\
                        & SCNN                & 35.1 & 14.2  \\
                        & Ra-SCNN             & 66.6 & 17.1  \\ 
   \bottomrule
 \end{tabular}
 %\caption{Classification accuracy of SCNN and Ra-SCNN models proposed in this work as compared to the convolutional neural network implementation of a filter bank common spatial pattern (FBCSP) classifier from \cite{Schirrmeister2017}.}
 \label{tab:sec_results}
\end{table}

The results in Table \ref{tab:sec_results} are meant as a verification that SCNN and its attention augmented counterpart are more generally applicable to MEG/EEG data and achieve results that are comparable to state-of-the-art end-to-end machine learning approaches. At first glance there are some increases in classification accuracy, although the augmented SCNN model performs relatively poorly with the augmented dataset. Performing paired Wilcoxon signed-rank tests of both SCNN and Ra-SCNN models with respect to the un-augmented CNN as FBCSP implementation, shows no significant improvement in performance. Although the SCNN model with cropping augmentation does show a significant decrease in performance with respect to the benchmark with $V_{8, one-sided}=45$ and $p<0.002$. It appears that these models are at least as good as a comparable state-of-the-art implementation in a completely unrelated task, excluding the SCNN with cropping augmentations.


\section*{Discussion}

This work proposes two novel deep neural network architectures that can distinguish between children at different levels of speech development with at least equivalent if not greater accuracy than a feature based-pipeline. We provide evidence that suggest that it is unlikely that classification decisions are made as a result of activity unrelated to the tasks themselves. It is also unlikely that decisions are solely made based on some age-correlated ERFs. The advantage of using these networks is their consolidated training process (they are entirely trained through gradient-descent based optimization without intermediate steps) and as greater steps towards explainable neural networks are made, their parameters may have direct and interpretable connections to the task they are employed against. They also perform quite comparably against the state-of-the-art in a completely separate BCI classification task, making them a general-purpose approach to many encephalographic data.

To model the fundamentally continuous nature of the phenomena that these data are meant to represent, we also considered targeting seven age categories, each two years in length (final category three years long), rather than the binary targets presented above. As might be expected, these do not yield accuracies as high as those from the binary models. However, as can be seen in figure \ref{fig:conf_fine_grained}, their performance does indicate that errors are dominated by confusion with nearby age categories, rather than randomly distributed. These results show promise that finer-grained contrasts can be made and future work will consider how best to deal with continuous phenomena like this. However, without further developing tools to suggest {\em why} a model decides to classify a trial as a certain category rather than another, we have limited our analysis in these work to the binary decision. Ensuring the greatest possible accuracy while using methods of model examination never previously explored with similar data is a prudent first step.

Towards determining if the /{\em pah}/ and /{\em pah tah kah}/ vocalization trials or the verb generation experiment trials had a greater impact on performance in the end-to-end learning, we trained the raw SCNN model using these datasets separately. Interestingly they seemed to have difficulty outperforming random chance on their own (in earlier model exploration). This suggests that despite being slightly different experiments perhaps at the very least the increase in training data is crucial to enabling the performance of these more powerful ML models.

As discussed in our methods, parameters such as the number of neurons, the receptive field and specific activation function were determined using a hyperparameter search. During this stage, we considered a finer-grained prediction of seven age targets, each two years in length. To model the more continuous nature of this target we employed {\em label-smoothing} \cite{Pereyra2017} where a target distribution is used rather than {\em one-hot} encoded labels for the seven classes. In practice this meant setting the true label to $0.6$ and its neighbouring classes to $0.2$ (and at the upper and lower ends we set the true label to $0.8$ with its neighbour again $0.2$). This showed some success, although future work will consider the internal model characteristics as accuracies improve. We also spent some time exploring CNN architectures that developed features using data across time and space, but found very little success with these methods, (although these experiments were preliminary, such that their parameters were selected heuristically). Interestingly the failures were generally not the result of over-fitting, as they found fitting to the training data surprisingly difficult (they could however memorize smaller subsets of the data). On the other hand, interpolating the channel data into a series of images rather than a series of channel vectors was {\em particularly} susceptible to overfitting. Unless considering datasets with many more training points, or employing some more effective regularization techniques we would not recommend proceeding with spatially projected data in future work. 

We train the early convolution layers in conjunction with the subsequent LSTM + attention layers for our attention augmented model. Previous work favours using pre-trained networks, but this is mostly due to the ubiquity of pre-trained image networks which make the need for re-training an image network unnecessary and time-consuming. This amounts to an added challenge of training a subset of the network to calculate a range of spatial and temporal filters, and then learn how to weight the sequence of these features. Future work would likely benefit from separating these tasks more to follow previous successes with attention mechanisms. For example the formulation of attention that we borrow from most \cite{Zhu}, is a transduction task that focuses on different aspects of a {\em pre-trained} CNN for image classification and then answers questions about the images using a LSTM+attention layer. This allows them to keep a relatively small training set (one that is less than one million examples) despite such a complex task. An interesting direction for future work might be to develop some general MEG/EEG applicable early layers out of the combination of many different open-access datasets, and then employ LSTMs with attention mechanisms for fine-tuning for a specific task.

Although no statistically significant conclusions can be made as to the power of the SCNN model versus its attention augmented counterpart, we should point out that when employing the cropping augmentation, particularly with the secondary dataset, and to a lesser extent primary task, the SCNN performs poorly as compared to the Ra-SCNN. This makes some intuitive sense, since the attention mechanism should allow for some insensitivity with respect to event offset, in a more powerful sense than the SCNN, which would rely on a later pooling layer for example to provide this functionality (and thus have a limited range). In practice however, the SCNN trained with cropping augmentation performed much better in the seven-way classification task than any other model.

As we mention below in our description of maximal activations, previous successes with this technique, such as in Yosinski {\em et al.}, use a heuristic combination of regularization methods. These penalties act as priors so that the artificial data fits the form of real data \cite{Yosinski2015}. An interesting example Yosinski {\em et al.} provide, is the use of Gaussian blur penalization that penalizes the production of images with high spatial frequency \cite{Yosinski2015}. We notice that in fact in our work, the spectrograms we calculate in fact do demonstrate some strangely distributed high frequency activity. It may be practical to not just begin with data that have $1/f$ spectral density as we do with our data, but to also penalize deviations from this expectation. There may be more that can be done to regularize artificial data and this warrants further study.

Future work that investigates maximal activations should also take steps to gauge event related synchrony/desynchrony in the context of artificial data, as the current consideration of spectrograms alone is fairly limited. Work in this direction may prove particularly interesting since convolution operations as implemented by most machine-learning libraries (including, with respect to our implementation, Tensorflow \url{https://www.tensorflow.org/}) in CNNs are in practice calculating a correlation measure \cite{GravesRNNBook}.

These results are interesting as a demonstration of the ability of end-to-end models to predict speech development, but neural networks show perhaps even greater promise as computational modelling techniques that to some degree mimic the human brain. For example \cite{cichy2016} compare the activations of an image classifier CNN and the MEG and fMRI activity of 15 subjects viewing the same test objects as the CNN. The architectures we present here are very general-purpose, given that they mimic the BCI pipeline, but future work should consider deep neural networks that mimic architectures that follow speech models with suggested neural substrates, such as the DIVA model\cite{Guenther2005}. These could be used to predict speech output or articulator positions with data such as ours, and explore their connection to the hypothesized brain structures.

\section*{Methods}

\subsection*{Primary dataset: Synchronized MEG and speech recordings}

\begin{table}[h]
  \centering
  \begin{tabular}{ l@{}c c c c c }
    \toprule
    \textbf{Stimuli} & \textbf{Mean Age} & \textbf{Age range} & \textbf{Subjects} & \textbf{Trials}  & \textbf{M/F Split} \\
    \midrule
    /{\em pah}/~~~                    & $10.73$ & $4.1-18.1$   &   $89$   &   $115$   &   $0.45/0.55$ \\
    /{\em pah tah kah}/~~~            & $11.01$ & $4.1-18.1$   &   $83$   &   $115$   &   $0.45/0.55$ \\
    VG~~~                             & $13.23$ & $5.7-18.0$   &   $28$   &   $81$    &   $0.42/0.58$  \\
    \bottomrule
  \end{tabular}
  \caption{Participant demographics, across stimuli types. The two speech tasks involve considerable participant overlap.}
  \label{tab:subjects}
\end{table}
These data were originally recorded to examine age- and sex-related developmental language differences in children by Doesburg {\em et al.} \cite{Doesburg2016} and Yu {\em et al.} \cite{Yu2014}. Table \ref{tab:subjects} summarizes participant demographics. Each participant spoke English as their first language and had no known or suspected histories of speech, language, hearing, or developmental disorders, according to their parents. The acquisition protocol was approved by the SickKids Research Ethics Board (REB \#1000016645) which acts in accordance with the guidelines established by the Tri-Council Policy on Ethical Conduct for Research Involving Humans. All participants or their parents gave written informed consent. Children unable to read the consent form provided informed verbal assent. Prior to the experiment, children received two standardized language clinical tests: the Peabody Picture Vocabulary Test (PPVT) \cite{Dunn97} and the Expressive Vocabulary Test (EVT) \cite{EVT}. All children's scores were at or above expected scores for their ages on the PPVT and EVT, and their speech showed neither signs of articulatory difficulties nor any significant effect of age. In total, 80 participants were right-handed, 5 were left-handed, and 7 were ambidextrous, according to the Edinburgh assessment \cite{Oldfield1971}, there is no significant variation of handedness with age. 

Three distinct speech-elicitation stimuli were used. The first two were of the monosyllable /{\em pah}/ and the multisyllabic sequence /{\em pah tah kah}/, respectively. These were simple enough for young children and are part of the diadochokinetic rate (DDK) test, which can be used to evaluate neuromuscular control in motor speech disorders. Prior to acquisition, the experimenter demonstrated the productions of each stimuli, without word-like prosodic patterns. A small white circle was displayed on the monitor as a cue to begin performing the speech-elicitation. The circle was present on the monitor for 200ms. The third experiment was an overt verb generation (VG) task in English, where subjects were presented with an image with which they were familiar, and were asked to produce a verb associated with the object \cite{Doesburg2016}. Image stimuli were presented to subjects for 500ms, and also served as cue to begin task. Between stimuli, participants were instructed to focus on a small white cross to minimize eye movements.

Recordings were made in a sound-proof room, with each participant lying supine in a magnetically shielded room in the Neuromagnetic Lab of the Hospital for Sick Children in Toronto, using a CTF whole-head MEG system (MEG International Services Ltd., Coquitlam, BC, Canada). To minimize movement with younger subjects, their heads are padded to restrict space, and they are given very clear instructions to stay still. If there is excess movement, the acquisition of trials is restarted, and if the subject continued to move the participant would be removed. We consider only subjects and recordings that have passed these requirements. The system recorded 151 MEG channels, and a single audio channel, with a sampling rate of 4 kHz. We perform a bare-minimum cleanup of the data were we resample MEG signals at 200 Hz, and band-pass filter between 0.5 Hz and 100 Hz, to remove offsets and accommodate the canonical ranges of $\delta$, $\theta$, $\alpha$, $\beta$, and $\gamma$ activity. Electrooculography (EOG) artifacts are removed using second order blind source identification as provided by EEGlab \cite{Delorme04eeglab}. Each trial consisted of 1.5 seconds before to 2 seconds after the onset of event cue, for trials of 3.5 seconds in length, with the exception of the training augmentation through cropping explained below.

Using these data, Doesburg {\em et al.} \cite{Doesburg2016} predicted language ability as an increase in network synchrony, particularly in the theta band with the increase of age during verb-generation (VG) tasks in children and adolescents. They observed a significant increase in the number of synchronous regions with older adolescents, compared with younger children. Yu {\em et al.} \cite{Yu2014} specifically focused on four regions of interest: inferior frontal regions, dorsolateral prefrontal regions, temporal-parietal regions, and superior temporal regions. They noticed distinct profiles of de-synchrony over time in VG tasks for children within five age ranges (i.e., 4-6, 7-9, 10-12, 13-15, and 16-18 years of age).

\subsection*{Supplementary dataset: BCI Competition IV, Dataset 2a}

In an effort to compare the potential success of our end-to-end models to previous work, we also trained some of our proposed models using the BCI Competition IV EEG Dataset \cite{Tangermann2012}. This dataset has been featured in several other attempts to apply neural networks trained end-to-end to neurophysiological data \cite{Schirrmeister2017,Tabar2017,Lawhern2017,Sun} and is freely available online: \url{http://bnci-horizon-2020.eu/database/data-sets}.

These data consist of EEG recordings of 9 subjects performing 4 different imagined motor tasks: left hand, right hand, both feet, and tongue. These recordings were broken up into two separate sessions for each subject recorded on different days. Each of these sessions consisted of 6 sets of 48 trials separated by a short break, where the 48 trials were 12 executions of each of the 4 tasks. Thus a total of 288 trials were recorded per session. One session is considered training data, and the second is used as evaluation requiring some carry-over performance between days. The recordings consist of 22 EEG electrodes, and 3 monopolar EOG electrodes, all recorded at 250 Hz and bandpass filtered between 0.5 and 100 Hz. Additionally, a 50 Hz notch-filter was used to minimize line noise. The trials themselves are available as 6 second recordings, where the first two seconds consist of presenting each subject a fixation point. At 2s through 3.25s, a task stimuli was presented to the subjects. There is additionally some EOG-only trials per session that we discard. 

\subsection*{Training Augmentation Through Cropping}

To augment the number of training points available, the entire trial is split into multiple training examples by taking subsections of each trial that still include the event onset. Effectively, rather than one training datapoint for each trial, a sliding window smaller than the length of the trial is used to crop many points, where each point has the event onset localized in a different place. The premise behind this augmentation is that with the event localized in different places, an architecture like a convolutional or recurrent network can learn temporal filters that are agnostic of a specific onset time and thus should be more generalizable. Previous work that has taken a similar approach include Schirrmeister {\em et al.} \cite{Schirrmeister2017} and Sun {\em et al.} \cite{Sun}, but alternatively to our work, they assume a convolutional input stage and provide variable temporal length training points rather than fixed length crops. We elected to use a sliding window of 2 seconds, which resulted in $(3.5s-2s) \times SamplingFrequency$ possible crop location per training point, or a possible augmentation of $300:1$.

\subsection*{Detailed neural network architectures}

\subsubsection*{Spatial summary convolutional neural network (SCNN)} \label{sec:scnn}

The critical innovation of the SCNN architecture is to enable multiple layers of non-linear spatial convolutions and temporal convolutions to be performed separately. This results in a set of spatial filtering components that can be examined independently from temporal features. This also mimics the feature-based approach pipeline of: (i) spatial (channel) mixing, (ii) applying filter banks to mixed components to develop a set of features, and then (iii) finally classifying with these features. In this architecture, the spatial convolutions ultimately span the length of the incoming channels, but rather than being a single linear layer, we introduce non-linearity and multiple convolution layers that work together to construct a more flexible spatial filter, but one that still employs weight sharing at each step. We select a hyperparameter indicating the depth of the spatial filter, and then stack convolutional layers, where each layer reduces the spatial dimension until the final layer completely collapses it. A number of temporal filters (i.e., convolutions over the temporal dimension) are then applied to the new spatial mappings (the number is also a selected hyperparameter), and an optional average pooling layer to reduce the number of parameters and effectively low-pass filter these new features. Finally, the resulting features are flattened into a single vector and a fully connected neural network (FFNN) classifier is used as an output stage, with the number of layers determined again through hyperparameter search.

The first few layers of the CNN can be seen as a mapping from $R^{T \times C \times 1}$ to $R^{T \times 1 \times S}$ (the spatial filtering) to $R^{T' \times 1 \times F }$ (the temporal filtering) where $T' \leq T$, $C$ is the number of recording channels, $S$ is the number of spatial transformations, and $F$ is the number of temporal filters. In other words, a set of $S$ non-linear spatial transformations are applied to the $C$ channels, and then a set of $F$ temporal filters are applied to these new sequences, as can be seen in the diagram presented in figure \ref{fig:scnn_arch}.

Since the actual depth (of spatial filtering and temporal filtering operations) changes depending on the configuration selected by the hyperparameter search, consider an example architecture with two layers of spatial steps with $K_1, K_2$ kernels for each layer respectively. Note that there are no bias terms included as batch normalization \cite{Szegedy2015} renders them redundant and is employed throughout. Let $\boldsymbol{X}$ be our input data with dimensions $T \times C$, where $T$ is the length of our temporal sequence and $C$ is the number of channels recorded. Also, as a result of the two spatial layers, we intend to produce the output $\boldsymbol{X}_{Spatial}$ with dimensions $T \times K_2$ where $K_2$ is the target number of {\em spatial components}. In the first layer, and $j^{th}$ kernel $\boldsymbol{w}_{L_1}^j$, has length $C'$, where $C' < C$, and the output of filter $j$ of the first layer at time $t$ is then:

\begin{equation} \label{eq:scnn_s1}
  \boldsymbol{X'}_{t, i, j} = f\left(\sum_{k=0}^{C'} {\boldsymbol{w}_{L_1}^j}_k \boldsymbol{X}_{t,i+k}\right)
\end{equation}

Where $\boldsymbol{X'}$ is a three dimensional tensor of shape $T \times (C-C') \times K_1$ and $f(x)$ is the neuron's non-linear function of choice. After the hyperparameter search detailed later, the SCNN model uses a scaled exponential linear function (SELU) \cite{NIPS2017_6698}. There is no integration of any temporal information besides $t$, and that for each kernel there are $C-C'$ sequences of length $T$, of which only the $i^{th}$ is shown above. Next, the second layer consists of kernel vectors $\boldsymbol{w}_{L_2}^s$ of length $C-C'$, which determine component $s$ of $K_2$ at time $t$ as follows:

\begin{equation} \label{eq:scnn_s2}
  {\boldsymbol{X}_{Spatial}}_{t, s} = f\left(\sum_{i=0}^{C-C'}{\boldsymbol{w}_{L_2}^s}_i \boldsymbol{X'}_{t, i, j}\right)
\end{equation}

The temporal filtering operations behave like eq. \ref{eq:scnn_s1}, but each kernel in the temporal layer is applied across the {\em temporal} dimension of each kernel activation sequence of the previous layer.

\subsubsection*{SCNN model augmented with attention focused recurrence (Ra-SCNN)} 

The Ra-SCNN is an extension of the SCNN architecture, that uses an LSTM and attention mechanism after the spatial and temporal steps to provide the network a stronger ability to focus on aspects of the signal. In our experiments, we employ an architecture (diagram shown in figure \ref{fig:alstm_arch}) that is very similar to the encoding stage employed in Zhu {\em et al.} for the purpose of image-directed question answering \cite{Zhu}. Those authors used a pre-trained CNN and an LSTM-based encoder that was fed attention weighted inputs. Their attention mechanism provides an average weighting of the different convolutional feature maps which are combined with one-hot encoded word vectors representing the words in the questions.

In contrast, our implementation does not use a pre-trained CNN, but trains convolutional layers at the same time as the rest of the model. The attention serves as a mechanism to closely consider parts of the sequence depending on the state of the LSTM. Where for some feature $j$ and point $t$ in a sequence of $T$, we calculate a new value for $j$ as the weighted sum of entire sequence of that feature.  

\begin{equation} 
  e_{t, j} = a(f_{0, j},..., f_{T-1, j}, h_{t-1}),
\end{equation}
Where $a()$ is an arbitrary function (we use a multi-layer network similar to the original formulation in \cite{Zhu}, but the number of layers used is a hyperparameter determined during search, and units per layer equal to the number of hidden units with the associated LSTM.). These vectors are normalized (so that they sum to 1, and range between 0 and 1) using the softmax function. This produces the {\em soft-attention} weights $\alpha_{t,j,t'}$ for each feature $j$ at time $t'$:

\begin{equation} \label{eq:attn_nrg}
  \alpha_{t,j,t'} =  \frac{\exp(e_{t,j,t'})}{\sum_{i=0}^{T-1}\exp(e_{t,j,i})},
\end{equation}

Finally the new attention-weighted feature $\hat{f}$ is: 

\begin{equation} \label{eq:attn}
    \hat{f_{t,j}} = \sum_{i=0}^{T} f_{i,j} \alpha_{t,j,i}
\end{equation}

So the intent is that the LSTM develops some sort of sequential processing that does not need to progress temporally sample by sample, but is flexible to consider any combination of samples that may become appropriate. In our case specifically, the attention mechanism pre-processes the output of the spatial summary and optionally (a hyperparameter) filtering over time.

\begin{equation}
  x_{t,i}^{In} = \sum_{t=0}^{T'} \alpha_{t,i} X_{t,i}^{SCNN}
\end{equation}

We select whether to use the full LSTM output sequence, or exclusively the last state output as part of our hyperparameter search, and optionally have several fully connected layers before the prediction layer.

\subsection*{Obscuring Profiles}

To produce figure \ref{fig:obsc_profile} we averaged {\em profiles} created for each test subjects in the test dataset, for each of the highest confidence (defined here as largest correct class output) test points within each task the subject performed. Each {\em profile} consists of 10 different uniform noise signals that grow from the event out to the ends in the case of {\em Obscure Event} or from the ends towards the event in {\em Obscure Ends}. Obscuring consists of a new point created out of the original signal alone for all points that are not obscured, and a weighted combination of the original signal and a noise signal, at a relative weighting of $1:1000$ respectively for the section that is obscured. This obscuring technique then {\em grows} by the addition of a single sample to the set of obscured points (and the respective removal of that point from the un-obscured set). For each sample obscured and each of the 10 noise signals, these modified points are fed to the pre-trained model and plotted. Since the event prompt is localized at $t=0.5s$ within a trial that is $3.5$ seconds long, the growing process proceeds at a rate of 6 samples to 1. Where 6 samples are added after the event at $t=0.5s$ for each added before in the case of {\em Obscure Event}, and 6 samples are added to the end of the trial for every 1 added to the beginning in the case of {\em Obscure Ends}.

\subsection*{Model analysis and visualizations} \label{sec:max_act}

Explaining {\em what} a trained neural network has learned is an ongoing area of research, and a particularly important one. For neural-network based models to be significantly useful beyond their successes as classifier tools, understanding what they are doing is crucial. We took an approach that generates artificial data by maximizing the outputs of a trained model with respect to artificial data (with no reliance on training points in particular), similar to that presented by Yosinski {\em et al.} \cite{Yosinski2015}. We selected this approach to focus on what type of input is preferred by key points in our models, and involves performing regularized gradient-ascent of an output $f_o(x)$ within the fully trained model with respect to the input $x$. So that we produce an artificial datapoint $x_{max}$ where:

\begin{equation} \label{eq:max_act}
  x_{max} = \argmaxA_x(f_o(x) - R(x))
\end{equation}

Here we summarize any regularization penalties as $R(x)$. Yosinski {\em et al.} show that crucial to the success of this technique is to ensure good prior distributions on the artificial data \cite{Yosinski2015}, so in this spirit we begin with randomly initialized data that have a spectral density that decreases with $1/f$ and zero mean, thus conforming to general encephalographic recordings. We also use L2 regularization on our data-points which helps prevent unbounded growth during the ascent which in effect prevents some strongly relevant features from eclipsing some features that are important but less impactful on the outputs. The iterative maximized input value is then:

\begin{equation} \label{eq:max_act_update}
  \hat{x}_{max} \leftarrow \eta \left(\hat{x}_{max} + \frac{\partial }{\partial x}(f_o(x) - \theta \cdot {}\sum_ix_i^2) \right)
\end{equation}

Where we heuristically select a step increase of $\eta = 0.2$ and L2 regularization of $\theta = 0.05$. We then iterate up to $10,000$ times, stopping if no progress is made for more than 5 steps. Additionally we normalize the derivative in eq. \ref{eq:max_act_update} for each step by its RMS value to make more stable (less oscillatory) progressions.

We construct maximal inputs for two sets of input-output pairs in the trained (using the primary dataset) end-to-end models. The first is between the model's normal input, and the end of the final-most spatial convolution layers, which we then interpolate (using bilinear interpolation) across true channel locations for our MEG machine to demonstrate a rough localization of spatial components/mixing. The second input-output pair is between the input to the first temporal convolution (the output of the last spatial) and the final model output (classification stage). This is to generate data that highlight temporal features of the patterns of mixed sensors, which we do by calculating a spectrogram (Hann windows with 50 samples overlap and 64 FFT bins) for each component and output class.

\subsection*{Comparison features}

For comparison, we perform a feature-engineering pipeline for use as a comparison against our proposed models. We first apply info-max independent component analysis (ICA) \cite{Bell1995} to determine statistically independent sub-components of the MEG recordings, across all subjects. These new components and a parallel recorded audio channel are then separated into windowed epochs corresponding to frames $-500$ ms to $+1500$ ms around the onset of the stimuli prompt. We extract 156 acoustic features and 4681 MEG features from each epoch using openSMILE \cite{Eyben13-RDI}. These are calculated using 50 ms rectangular windows, with a 25 ms overlap resulting in 79 windows per datapoint. Features to represent spectral activity are calculated for each window, using a fast Fourier transform (128 points for the 4kH{\em z} audio recordings, and 8 points for the 200H{\em z} MEG channels)) and linear predictive coding coefficients. Additionally, the summary statistics: mean (also absolute mean, quadratic mean, and aforementioned means calculated using only non-zero values), variance, skewness, and kurtosis are calculated. Finally, the root-mean-squared and log of the signal energy are also calculated for each window. Unique to the MEG channels we also calculate an autocorrelation function (ACF) calculated using the fast Fourier transform (FFT) and its inverse (iFFT) for window $w$:

\begin{equation}
  ACF(w) = iFFT(|FFT(w)|^2)
  \label{eq1}
\end{equation}

As a result of the number of features extracted for each MEG trial, the number of features greatly outnumbers the number of training points available. As this reduces the efficacy of convex optimizations, we first select a sub-set of features to use during the training process. We use standard Pearson correlation between the MEG features and age, and highlight 172 features with $p$-values at $\alpha = 10^{-5}$ after Bonferroni correction, and correlation coefficients $\text{abs}(r) > 0.2$. These 172 features include the autocorrelation features of eight ICA components (including the first (highest entropy) component) in addition to the entire available frequency spectrum of four of these components.

\subsection*{Model training procedures} \label{sec:train_proc}

Each model is tested using 5-fold cross-validation against a held-out group of 9 subjects, only one of which did not perform the verb generation task. The test subjects are selected to have a similar distribution of the number of trials in each age range. The remaining subjects are ordered based on the number of trials they performed and their trials are distributed across the 5 folds in this order to create an approximately equal number of trials in each fold. The range in number of training points between folds was 3838 through 4078. We quantitatively evaluate the similarity between the 5 folds and test set using a K-sample Anderson-Darling test, where we found $A^2=-0.271$ with $p=0.61$. In the supplemental dataset, the training versus test datasets are separated in advance for intra-subject training. Due to the imbalance of points between verb-generation and the other two tasks, a loss penalty was applied to each output class equal to $1-\frac{n_{class}}{n_{total}}$ during all training stages of the task prediction experiments.

We use Keras (\url{https://keras.io/}) with a Tensorflow (\url{https://www.tensorflow.org/}) backend to build all of the following models, and use either stochastic gradient descent or the Adam optimizer\cite{Kingma2015} when performing back-propagation updates. When training the neural networks, we select between three different activation functions: the rectified linear unit (ReLU) \cite{He2015a}, the exponential linear unit (ELU) \cite{Clevert}, and the scaled ELU (SELU) \cite{NIPS2017_6698}. All layers are batch-normalized \cite{Szegedy2015} after activation. We additionally employ L2-norm weight regularization for all trainable weights, excluding those of RNNs. We apply dropout \cite{Srivastava2014} after all fully-connected layers, and both max-pooling and spatial dropout \cite{Tompson2015} to all convolutional layers. These techniques are not necessarily applied to the FBCSP-like model from \cite{Schirrmeister2017}, which is reproduced by examining their source code.

All hyperparameter selection is done using the tree-structured Parzen estimator implemented in Hyperopt \cite{Bergstra2013} including: learning rate, regularization penalty, dropout rates, number of adjacent activations to pool, number and layers of hidden units, receptive fields, and activation functions for all appropriate models. Hyperparameter searches are performed for 100 iterations.

\bibliography{sci_reports}

\section*{Acknowledgements}

We acknowledge Drs. Darren Kadis and Vickie Yu who developed the MEG stimuli and determined the MEG data acquisition parameters. We acknowledge Matt J. MacDonald, Anna Oh and Marc Lalancette for their roles in recruiting the MEG participants, acquiring the MEG data and standardized language scores. The MEG portion of this study was funded by a Canadian Institutes of Health Research Operating Grant (MOP-89961) to Elizabeth W. Pang.

\section*{Author contributions statement}

E.P. directed collection of the primary dataset, F.R. conceived experiment and reworked draft manuscript, D.K conducted experiments, analyzed results and wrote initial manuscript, D.K. and F.R. developed proposed models. All authors edited and reviewed manuscript.

\section*{Competing interests}

The author(s) declare no competing interests.

\section*{Access to collected data and source code}

All source code will be made available publicly. For access to the data, please contact the corresponding author.

\end{document}