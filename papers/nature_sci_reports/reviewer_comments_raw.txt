Reviewer comments:

Reviewer #1 (Technical Comments to the Author):

The manuscript describes the development of two convolutional neural network architectures to classify child age from MEG recordings during speech tasks.

I found the technical description of the algorithms to be well written, and the authors seem to have expertise in training and evaluating networks. They show good and extensive data analysis that is innovative in the realm of MEG data.

However, I found that the main claim of the manuscript is unsupported: that these networks are predicting language development. The authors have built good classifiers for the age of the subject. The relationship between age, language development, and classifier performance is speculative.


Reviewer #1 (Remarks to the Author):

The authors developed two algorithms for classifying the age of subjects (>=10 or <10) from their MEG recordings while they engaged in three speech tasks. They proposed CNNs that summarize the spatio-temporal patterns in MEG data (one of which incorporate attention), and show that they can perform significantly better than state of the art classifiers.

The performance of the algorithms is encouraging and the proposed networks seem to be intelligently developed for the problem at hand. the authors applied and extended multiple current approaches for training neural networks and seem to have good expertise in the subject.

However it seems to me that the potential of these methods is diluted in the task they are used for. What is the use of predicting age from MEG signals when it can be so easily predicted by much simpler metrics? If practical applications is not the goal, but the goal is a more scientific understanding of speech development, then I am not sure how a coarse metric like "age >=10" relates to it. What the algorithm is detecting brain features that are different across children of age >=10 or <10. I am not convinced that the features detected are relevant to speech development.

I suggest that the authors perform some of these checks in order to make their conclusions more impactful:
- try to predict a closer measure of language development if such a measure was recorded
- try to predict the actual production task (which of the three production tasks each trial corresponds to) instead of age to check their algorithms: the artificial data that the authors simulate by maximizing the classifier output shows that the time dimension is not effectively utilized by the network. It seems to me from the manuscript that the networks are rather utilizing frequency band information that is different in older children from younger children. In order to see if the temporal filtering detects temporal patterns (relevant to stimulus onset and offset etc.), training the networks to distinguish between trials might showcase the power of these networks at learning temporal MEG patterns. Note: this could have happened in the BCI tasks that were used, but these results were not expanded on.
- if the reason why the networks have a good accuracy is that they are sensitive to the difference in frequency bands, then it's important to know if these frequency bands are related to the tasks or not. If possible, the analysis can be repeated with the portion of the data before (and after) the actual onset of the spoken words. If the accuracy persists to be above chance, then maybe what is being classified is a baseline activity and not something about speech production.
- figure 2 shows that the most relevant channels are typically the ones on the outer sides of the helmet. There is a hot spot on the upper left which could be related to left inferior frontal cortex, but of course it's hard to tell with MEG. One thing I was wondering about is how important head size is. Is it (partially) what is being classified? Head size, or movement, could easily be correlated to age. Can the authors think of additional tests to run that would clarify this issue?


Additional comments on clarity:
- This paper is well written in terms of explaining the techniques used.
- The organization however makes it hard to read. Even though the Results section should be after the Introduction, there should be some quick mention in there of the experimental task and what is being classified. Otherwise it's hard to make sense of the paper without going back and forth.
- the technical details are many. It would be good to have a diagram explaining the different analysis that were done (maybe a box/arrows diagram).
- the label sizes in figure 3 are too small
- it's a little unclear whether the classification is ultimately done on a single trial basis. If so, this should be highlighted since the classification results would be more significant, because single trial data is typically very noisy and hard to classify
- in the discussion section "On the other hand the spatial projection dataset was particularly susceptible to overfitting to the training data. " it's confusing what projection is being referred to.


Reviewer #2 (Remarks to the Author): 

This paper uses deep neural networks for predicting the age of children from MEG data recorded during speech tasks. In the proposed SCNN architecture, spatial and temporal correlations are performed separately. Another version of the architecture, called Ra-SCNN, uses also LTM and attention mechanisms. The networks were reported to classify the age to be below or above 10 years with over 97% accuracy. The weights of the trained models were analyzed to provide insights what aspects of the brain data were most relevant for the classification. 

The proposed neural networks provide interesting end-to-end models for the MEG data. Unfortunately, the organization of the paper is such that it is difficult to follow what exactly was done. To be able to properly evaluate the results, the text should be substantially revised. I can understand that the format of putting Methods section in the end poses a challenge to this type of paper. However, the authors should try to make the text flow better, explaining the terminology and main points of the methods as needed when presenting (or discussing) the results. For example, Figure 1 is placed on page 2, but referred to in the text only on page 6. Similarly, Tables 1 and 2 contain a lot of acronyms that are not explained. Since the networks are a crucial part of the paper, the novel aspects should be indicated in the Introduction. 

Specific comments: 

!!! Abstract: please spell out all acronyms (fMRI, MEG, DNN). Similarly throughout the manuscript, spell out acronyms when they first appear. 

!!! Figure 1: explain what is indicated by the different colors 

Results, under “Age classification” (page 2): 
-- line 1: it has not been explained which ones are “end-to-end approaches” in Table 1 
!!! -- line 2: “un-augmented raw data” has not been explained 
!!! -- line 3: “accuracy is over 97%” I can’t see this in Table 1. 

Results, under “Secondary dataset classification” (page 2): 
!!! -- line 5: “excluding the dramatic outlier” Which one is this? How is the exclusion justified? 

Results, under “Model analysis” (page 3): 
-- line 4: “interpolated over a scalp image” is not clear. Was the data extrapolated from the MEG sensors to scalp surface? 
-- line 4: what are the “three spatial features” that are maximized? 
-- 2nd paragraph, line 3: “inverse sign” What is the interpretation of this? 

!!! Figure 2: Please indicate the color scale. Also, a “title” for the figure, simply saying what is actually shown, would be helpful. 

Discussion, page 4: 
-- 4th para, line 1: “the seven-way classification task” What task was that?
>>> Before focusing on producing a very accurate classifier, and examining its workings. Our original task consider a few other models and a finer grained set of age groups. Some of this work was written in conjunction and should not have made it into this manuscript.

Figure 3: What do the colors represent? (Weights? Signal power? What are the units?). Also, the font size in the panels is too small to be readable.
>>> The signal colours represent unitless input data (as all inputs are z-score normalized, and there is batch-normalization happening at each layer of the network). We have reworked the beginning of the "Model Activations" section to clarify this.

Proposed neural network architectures 
-- page 6, below Eq. (1): “non-linear function of choice” What function was used in this study?
>>> The section "Model Training Procedures" does make it clear that a hyper-parameter search is used to determine which of three activation works best for a task, but we have added an ind     ication of which activation is used for the final architecture selected where there was some confusion. 

Methods 
-- page 8, line 5 under “Sensor projection: what exactly is the “grid sized h x v”? How were “h” and “v” determined? Does the projection grid mean that MEG sensors in the middle of the head were considered to be closer to each other spatially than the temporal and occipital sensors?
>>> This was discussing a model that we initially considered before some switch of focus. We believe we have now removed all artifacts of this previous draft.

-- page 9, line 3 under “Cropping”: “used to crop many points” I don’t understand the use of the word “crop” here. Is this standard terminology?
>>> Cropping is fairly standard terminology for cutting a subsection of audio or image

-- page 9, line 5 under “Adding sensor noise”: “truncated within a single deviation” What is a “single deviation”?
>>> Another artifact of work that should not have been mentioned here
-- page 9, line 9 under “Adding sensor noise”: “sensor projection of course increases the dimensionality of the data significantly” Why is that? Isn’t the number of sensors still the same, otherwise the data is redundant?
>>> Same as previous point


