Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Hochreiter1997a,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, Jj},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hochreiter, Urgen Schmidhuber - 1997 - LONG SHORT-TERM MEMORY(2).pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{LONG SHORT-TERM MEMORY}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen http://www7.informatik.tu-muenchen.de/{~}hochreit{\%}5Cnhttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{XuKELVINXU,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {1502.03044},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu KELVINXU et al. - Unknown - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
pmid = {18267787},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {https://arxiv.org/pdf/1502.03044.pdf http://arxiv.org/abs/1502.03044},
year = {2015}
}
@inproceedings{Bashivan2016,
abstract = {One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter-and intra-subject differences, as well as to inherent noise associated with EEG data collection. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification techniques to learn robust representations from the sequence of images. The proposed ap-proach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06448v3},
author = {Bashivan, Pouya and Rish, Irina and Yeasin, Mohammed and Codella, Noel},
booktitle = {ICLR 2016},
eprint = {arXiv:1511.06448v3},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bashivan et al. - Unknown - LEARNING REPRESENTATIONS FROM EEG WITH DEEP RECURRENT-CONVOLUTIONAL NEURAL NETWORKS.pdf:pdf},
isbn = {9783901608353},
pages = {1--15},
title = {{Learning Representations From Eeg With Deep Recurrent-Convolutional Neural Networks}},
url = {https://arxiv.org/pdf/1511.06448.pdf},
year = {2016}
}
@article{RezaeiTabar2016,
abstract = {Brain Computer Interface (BCI) systems provide control of external devices by using only brain activity. In recent years, there has been a great interest in developing BCI systems for different applications. These systems are capable of solving daily life problems for both healthy and disabled people. One of the most important applications of BCI is to provide communication for disabled people that are totally paralysed. In this paper, different parts of a BCI system and different methods used in each part are reviewed. Neuroimaging devices, with an emphasis on EEG (electroencephalography), are presented and brain activities as well as signal processing methods used in EEG-based BCIs are explained in detail. Current methods and paradigms in BCI based speech communication are considered.},
author = {{Rezaei Tabar}, Yousef and Halici, Ugur},
doi = {10.1017/S1062798716000569},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Brain Computer Interfaces for Silent Speech.pdf:pdf},
isbn = {1062798716},
issn = {1062-7987},
journal = {European Review},
number = {02},
pages = {208--230},
title = {{Brain Computer Interfaces for Silent Speech}},
url = {https://www-cambridge-org.myaccess.library.utoronto.ca/core/services/aop-cambridge-core/content/view/F6100C30A5680AACD9270D5E2ACEF5A1/S1062798716000569a.pdf/brain{\_}computer{\_}interfaces{\_}for{\_}silent{\_}speech.pdf https://www.cambridge.org/core/product/identifier/},
volume = {25},
year = {2016}
}
@article{Yu2014,
author = {Yu, Vickie Y and Macdonald, Matt J and Oh, Anna and Hua, Gordon N and Nil, Luc F De and Elizabeth, W and Pang, Elizabeth W},
doi = {10.1037/a0037470},
file = {:home/demetres/Documents/Research Summer 2016/Yu et al{\_}Dev Psyc 2014.pdf:pdf},
issn = {00121649},
keywords = {adult,dominance is lateralized to,event-related desynchroni-,gamma oscillations,in a typical right-handed,inferior frontal gyrus,it is well accepted,magnetoencephalography,that language,the brain and that,the left hemisphere of,verb generation,zation},
number = {9},
pages = {2276--2284},
title = {{Developmental Psychology Age-Related Sex Differences in Language Lateralization : A Magnetoencephalography Study in Children Age-Related Sex Differences in Language Lateralization : A Magnetoencephalography Study in Children}},
volume = {50},
year = {2014}
}
@article{Nakasaki1989,
abstract = {Following the discovery of context-dependent synchronization of oscillatory neuronal responses in the visual system, the role of neural synchrony in cortical networks has been expanded to provide a general mechanism for the coordination of distributed neural activity patterns. In the current paper, we present an update of the status of this hypothesis through summarizing recent results from our laboratory that suggest important new insights regarding the mechanisms, function and relevance of this phenomenon. In the fi rst part, we present recent results derived from animal experiments and mathematical simulations that provide novel explanations and mechanisms for zero and nero-zero phase lag synchronization. In the second part, we shall discuss the role of neural synchrony for expectancy during perceptual organization and its role in conscious experience. This will be followed by evidence that indicates that in addition to supporting conscious cognition, neural synchrony is abnormal in major brain disorders, such as schizophrenia and autism spectrum disorders. We conclude this paper with suggestions for further research as well as with critical issues that need to be addressed in future studies.},
annote = {NULL},
author = {Uhlhaas, Peter},
doi = {10.3389/neuro.07.017.2009},
file = {:home/demetres/Downloads/017{\_}2009.pdf:pdf},
isbn = {1662-5145 (Electronic)},
issn = {16625145},
journal = {Frontiers in Integrative Neuroscience},
keywords = {cognition,cortex,gamma,oscillations,synchrony},
number = {13},
pages = {3662--3669},
pmid = {2659167},
title = {{Neural synchrony in cortical networks: history, concept and current status}},
url = {http://journal.frontiersin.org/article/10.3389/neuro.07.017.2009/abstract},
volume = {3},
year = {2009}
}
@article{Bahdanaua,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - Unknown - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(3).pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {https://arxiv.org/pdf/1409.0473.pdf http://arxiv.org/abs/1409.0473},
year = {2014}
}
@article{Sun,
abstract = {Prediction of memory performance (remembered or forgotten) has various potential applica-tions not only for knowledge learning but also for disease diagnosis. Recently, subsequent memory effects (SMEs)—the statistical differences in electroencephalography (EEG) sig-nals before or during learning between subsequently remembered and forgotten events— have been found. This finding indicates that EEG signals convey the information relevant to memory performance. In this paper, based on SMEs we propose a computational approach to predict memory performance of an event from EEG signals. We devise a convolutional neural network for EEG, called ConvEEGNN, to predict subsequently remembered and for-gotten events from EEG recorded during memory process. With the ConvEEGNN, predic-tion of memory performance can be achieved by integrating two main stages: feature extraction and classification. To verify the proposed approach, we employ an auditory mem-ory task to collect EEG signals from scalp electrodes. For ConvEEGNN, the average predic-tion accuracy was 72.07{\%} by using EEG data from pre-stimulus and during-stimulus periods, outperforming other approaches. It was observed that signals from pre-stimulus period and those from during-stimulus period had comparable contributions to memory per-formance. Furthermore, the connection weights of ConvEEGNN network can reveal promi-nent channels, which are consistent with the distribution of SME studied previously.},
author = {Sun, Xuyun and Qian, Cunle and Chen, Zhongqin and Wu, Zhaohui and Luo, Benyan and Pan, Gang},
doi = {10.1371/journal.pone.0167497},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - Unknown - Remembered or Forgotten—An EEG-Based Computational Prediction Approach.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
title = {{Remembered or forgotten?-An EEG-Based computational prediction approach}},
url = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0167497{\&}type=printable},
volume = {11},
year = {2016}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {ICLR},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{Doesburg2016,
abstract = {Synchronization of oscillations among brain areas is understood to mediate network communication supporting cognition, perception, and language. How task-dependent synchronization during word production develops throughout childhood and adolescence, as well as how such network coherence is related to the development of language abilities, remains poorly understood. To address this, we recorded magnetoencephalography while 73 participants aged 4–18 years performed a verb generation task. Atlas-guided source reconstruction was performed, and phase synchronization among regions was calculated. Task-dependent increases in synchronization were observed in the theta, alpha, and beta frequency ranges, and network synchronization differences were observed between age groups. Task-dependent synchronization was strongest in the theta band, as were differences between age groups. Network topologies were calculated for brain regions associated with verb generation and were significantly associated with both age and language abilities. These findings establish the maturational trajectory of network synchronization underlying expressive language abilities throughout childhood and adolescence and provide the first evidence for an association between large-scale neurophysiological network synchronization and individual differences in the development of language abilities.},
annote = {NULL},
author = {Doesburg, Sam M. and Tingling, Keriann and MacDonald, Matt J. and Pang, Elizabeth W.},
doi = {10.1162/jocn_a_00879},
file = {:home/demetres/Downloads/jocn{\_}a{\_}00879.pdf:pdf},
journal = {Journal of Cognitive Neuroscience},
month = {jan},
number = {1},
pages = {55--68},
title = {{Development of Network Synchronization Predicts Language Abilities}},
url = {http://arxiv.org/abs/1511.04103 http://www.mitpressjournals.org/doi/10.1162/jocn{\_}a{\_}00879},
volume = {28},
year = {2016}
}
@article{Szegedy2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {https://arxiv.org/pdf/1502.03167.pdf http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Guger2000,
abstract = {—Electroencephalogram (EEG) recordings during right and left motor imagery allow one to establish a new com-munication channel for, e.g., patients with amyotrophic lateral sclerosis. Such an EEG-based brain–computer interface (BCI) can be used to develop a simple binary response for the control of a device. Three subjects participated in a series of on-line sessions to test if it is possible to use common spatial patterns to analyze EEG in real time in order to give feedback to the subjects. Furthermore, the classification accuracy that can be achieved after only three days of training was investigated. The patterns are estimated from a set of multichannel EEG data by the method of common spatial patterns and reflect the specific activation of cortical areas. By construction, common spatial patterns weight each electrode according to its importance to the discrimination task and suppress noise in individual channels by using corre-lations between neighboring electrodes. Experiments with three subjects resulted in an error rate of 2, 6 and 14{\%} during on-line discrimination of left-and right-hand motor imagery after three days of training and make common spatial patterns a promising method for an EEG-based brain–computer interface. Index Terms—Brain–computer interface (BCI), common spatial patterns (CSP), event-related desynchronization (ERD), real-time software.},
author = {Guger, C and Ramoser, H and Pfurtscheller, G},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guger, Ramoser, Pfurtscheller - 2000 - Real-Time EEG Analysis with Subject-Specific Spatial Patterns for a Brain–Computer Interface (B.pdf:pdf},
journal = {IEEE TRANSACTIONS ON REHABILITATION ENGINEERING},
number = {4},
pages = {447--456},
title = {{Real-Time EEG Analysis with Subject-Specific Spatial Patterns for a Brain – Computer Interface (BCI)}},
url = {https://journals-scholarsportal-info.myaccess.library.utoronto.ca/pdf/10636528/v08i0004/447{\_}reawsspfabi.xml},
volume = {8},
year = {2000}
}
@article{Blankertz2007,
abstract = {Brain-Computer Interface (BCI) systems establish a direct communication channel from the brain to an output device. These systems use brain signals recorded from the scalp, the surface of the cortex, or from inside the brain to enable users to control a variety of applications. BCI systems that bypass conventional motor output pathways of nerves and muscles can provide novel control options for paralyzed patients. One classical approach to establish EEG-based control is to set up a system that is controlled by a specific EEG feature which is known to be susceptible to conditioning and to let the subjects learn the voluntary control of that feature. In contrast, the Berlin Brain-Computer Interface (BBCI) uses well established motor competencies of its users and a machine learning approach to extract subject-specific patterns from high-dimensional features optimized for detecting the user's intent. Thus the long subject training is replaced by a short calibration measurement (20 min) and machine learning (1 min). We report results from a study in which 10 subjects, who had no or little experience with BCI feedback, controlled computer applications by voluntary imagination of limb movements: these intentions led to modulations of spontaneous brain activity specifically, somatotopically matched sensorimotor 7-30 Hz rhythms were diminished over pericentral cortices. The peak information transfer rate was above 35 bits per minute (bpm) for 3 subjects, above 23 bpm for two, and above 12 bpm for 3 subjects, while one subject could achieve no BCI control. Compared to other BCI systems which need longer subject training to achieve comparable results, we propose that the key to quick efficiency in the BBCI system is its flexibility due to complex but physiologically meaningful features and its adaptivity which respects the enormous inter-subject variability. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Blankertz, Benjamin and Dornhege, Guido and Krauledat, Matthias and M{\"{u}}ller, Klaus- Robert and Curio, Gabriel},
doi = {10.1016/j.neuroimage.2007.01.051},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blankertz et al. - 2007 - The non-invasive Berlin Brain–Computer Interface Fast acquisition of effective performance in untrained s(2).pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
number = {2},
pages = {539--550},
pmid = {17475513},
title = {{The non-invasive Berlin Brain-Computer Interface: Fast acquisition of effective performance in untrained subjects}},
url = {https://journals-scholarsportal-info.myaccess.library.utoronto.ca/pdf/10538119/v37i0002/539{\_}tnbbifoepius.xml},
volume = {37},
year = {2007}
}
@article{Bahdanaub,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - Unknown - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(3).pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {https://arxiv.org/pdf/1409.0473.pdf http://arxiv.org/abs/1409.0473},
year = {2014}
}
@article{VanDenOord,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
doi = {10.1109/ICASSP.2009.4960364},
eprint = {1609.03499},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Den Oord et al. - Unknown - WAVENET A GENERATIVE MODEL FOR RAW AUDIO.pdf:pdf},
isbn = {9783901882760},
issn = {0899-7667},
pmid = {18785855},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {https://arxiv.org/pdf/1609.03499v2.pdf http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{Fries2015,
abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30-90 Hz) synchronization modulates excitation rapidly enough that it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g., representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up-directed gamma-band influences are controlled by predominantly top-down-directed alpha-beta-band (8-20 Hz) influences. Attention itself samples stimuli at a 7-8 Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise, and selective.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Fries, Pascal},
doi = {10.1016/j.neuron.2015.09.034},
eprint = {15334406},
file = {:home/demetres/Downloads/PIIS0896627315008235.pdf:pdf},
isbn = {http://dx.doi.org/10.1016/j.neuron.2015.09.034},
issn = {10974199},
journal = {Neuron},
number = {1},
pages = {220--235},
pmid = {26447583},
publisher = {Elsevier Inc.},
title = {{Rhythms for Cognition: Communication through Coherence}},
url = {http://dx.doi.org/10.1016/j.neuron.2015.09.034},
volume = {88},
year = {2015}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey and Y., Lecun and Y., Bengio and G., Hinton},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/demetres/Documents/Papers/Computers/ML/NatureDeepReview.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Yu2015,
abstract = {Voice onset time (VOT) is a temporal acoustic parameter that reflects motor speech coordination skills. This study investigated the patterns of age and sex differences across development of voice onset time in a group of 70 English-speaking children, ranging in age from 4.1 to 18.4 years, and 12 young adults. The effect of the number of syllables on VOT patterns was also examined. Speech samples were elicited by producing syllables /pa/ and /pataka/. Results supported previous findings showing that younger children produce longer VOT values with higher levels of variability. Markedly higher VOT values and increased variability were found for boys at ages between 8 and 11 years, confirming sex differences in VOT patterns and patterns of variability. In addition, all participants consistently produced shorter VOT with higher variability for multisyllables than monosyllables, indicating an effect of syllable number. Possible explanations for these findings and clinical implications are discussed.},
annote = {NULL},
author = {Yu, Vickie. Y. and {De Nil}, L. F. and Pang, E. W.},
doi = {10.1177/0023830914522994},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, De Nil, Pang - 2015 - Effects of Age, Sex and Syllable Number on Voice Onset Time Evidence from Children's Voiceless Aspirated Stops.pdf:pdf},
issn = {0023-8309},
journal = {Language and Speech},
keywords = {Voice onset time,sex differences,syllable number,variability patterns},
number = {2},
pages = {152--167},
title = {{Effects of Age, Sex and Syllable Number on Voice Onset Time: Evidence from Children's Voiceless Aspirated Stops}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930468626{\&}partnerID=tZOtx3y1},
volume = {58},
year = {2015}
}
@article{Tangermann2012,
abstract = {The BCI competition IV stands in the tradition of prior BCI competitions that aim to provide high quality neuroscientific data for open access to the scientific community. As experienced already in prior competitions not only scientists from the narrow field of BCI compete, but scholars with a broad variety of backgrounds and nationalities. They include high specialists as well as students. The goals of all BCI competitions have always been to challenge with respect to novel paradigms and complex data. We report on the following challenges: (1) asynchronous data, (2) synthetic, (3) multi-class continuous data, (4) session-to-session transfer, (5) directionally modulated MEG, (6) finger movements recorded by ECoG. As after past competitions, our hope is that winning entries may enhance the analysis methods of future BCIs.},
author = {Tangermann, Michael and M{\"{u}}ller, Klaus Robert and Aertsen, Ad and Birbaumer, Niels and Braun, Christoph and Brunner, Clemens and Leeb, Robert and Mehring, Carsten and Miller, Kai J. and M{\"{u}}ller-Putz, Gernot R. and Nolte, Guido and Pfurtscheller, Gert and Preissl, Hubert and Schalk, Gerwin and Schl{\"{o}}gl, Alois and Vidaurre, Carmen and Waldert, Stephan and Blankertz, Benjamin},
doi = {10.3389/fnins.2012.00055},
file = {:home/demetres/Downloads/fnins-06-00055.pdf:pdf},
isbn = {1662-453X},
issn = {16624548},
journal = {Frontiers in Neuroscience},
keywords = {Bci,Brain-computer interface,Competition},
number = {JULY},
pages = {1--31},
pmid = {22811657},
title = {{Review of the BCI competition IV}},
volume = {6},
year = {2012}
}
@article{Poulos2001,
abstract = {Person identification based on spectral information extracted from the EEG is addressed in this work a problem that has not yet been seen in a signal processing framework. Spectral features are extracted non-parametrically from real EEG data recorded from healthy individuals. Neural network classification is applied on these features using a Learning Vector Quantizer in an attempt to experimentally investigate the connection between a person's EEG and genetically specific information. The proposed method, compared with previously proposed methods, has yielded encouraging correct classification scores in the range of 80{\%} to 100{\%} (case-dependent). These results are in agreement with previous research showing evidence that the EEG carries genetic information},
annote = {NULL},
author = {Poulos, M and Rangoussi, M and Alexandris, N and Evangelou, A},
doi = {10.1080/14639230118937},
file = {:home/demetres/Downloads/On the use of EEG features towards person identification via neural networks.pdf:pdf},
isbn = {1463923001001},
issn = {1753-8157},
journal = {Med Inform.Internet.Med},
keywords = {Adult,Alpha Rhythm,Anthropology,Physical,Beta Rhythm,Classification,Electroencephalography,False Negative Reactions,False Positive Reactions,Female,Fourier Analysis,Human,Learning,Male,Medical Informatics Applications,Medical Informatics Computing,Middle Aged,Neural Networks (Computer),Patient Identification Systems,Pedigree,Sensitivity and Specificity,Signal Processing,Computer-Assisted,Support,Non-U.S.Gov't,Theta Rhythm,methods,neural},
number = {1},
pages = {35--48},
title = {{On the use of EEG features towards person identification via neural networks}},
volume = {26},
year = {2001}
}
@article{Bergstra2013,
abstract = {—Sequential model-based optimization (also known as Bayesian op-timization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for per-forming hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. The paper closes with some discussion of ongoing and future work.},
author = {Bergstra, James and Yamins, Dan and Cox, David D},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Yamins, Cox - 2013 - Hyperopt A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms.pdf:pdf},
journal = {PROC. OF THE 12th PYTHON IN SCIENCE CONF},
keywords = {Index Terms—Bayesian optimization,hyperparameter optimization,model se-lection},
pages = {13--20},
title = {{Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms}},
url = {http://www.youtube.com/watch?v=Mp1xnPfE4PY},
year = {2013}
}
@article{Lawhern2017,
abstract = {Objective: Brain-Computer Interface (BCI) technologies enable direct communication between humans and computers by analyzing brain measurements, such as electroencephalography (EEG). BCI processing typically consists of heuristically extracting features for specific tasks, limiting the generalizability of the BCI across tasks. Here, we asked whether we can find a single generalized neural network architecture that can accurately classify EEG signals in different BCI tasks. Approach: In this work we introduce EEGNet, a compact fully convolutional network for EEG-based BCIs. We compare EEGNet to the current state-of-the-art approach across four different BCI classification tasks: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). We fit 12 different architectures, all with the same number of parameters, to statistically control for the effect of model size versus model performance. Results: We show that one particular architecture performed on average the best over all datasets, suggesting that a generic model can be used for a variety of BCIs. We also show that EEGNet compares favorably to the current best state-of-the-art approach for each dataset across all four datasets. Significance: Our findings suggest that a common simplified architecture, EEGNet, can provide robust performance across many different BCI modalities.},
archivePrefix = {arXiv},
arxivId = {1611.08024},
author = {Lawhern, Vernon J and Solon, Amelia J and Waytowich, Nicholas R and Gordon, Stephen M and Hung, Chou P and Lance, Brent J},
doi = {10.1371/journal.pone.0138297},
eprint = {1611.08024},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lawhern et al. - 2017 - EEGNet A Compact Convolutional Network for EEG-based Brain-Computer Interfaces(2).pdf:pdf},
issn = {1932-6203},
title = {{EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces}},
url = {https://arxiv.org/pdf/1611.08024.pdf http://arxiv.org/abs/1611.08024},
year = {2017}
}
@incollection{NIPS2017_6698,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
booktitle = {Advances in Neural Information Processing Systems 30},
doi = {1706.02515},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
eprint = {1706.02515},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klambauer et al. - Unknown - Self-Normalizing Neural Networks.pdf:pdf},
pages = {972--981},
publisher = {Curran Associates, Inc.},
title = {{Self-Normalizing Neural Networks}},
url = {http://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf http://arxiv.org/abs/1706.02515},
year = {2017}
}
@inproceedings{Krell2017,
abstract = {For deep learning on image data, a$\backslash$ncommon approach is to augment the training data by artificial$\backslash$nnew images, using techniques like moving windows, scaling,$\backslash$naffine distortions, and elastic deformations. In contrast to image$\backslash$ndata, electroencephalographic (EEG) data suffers even more$\backslash$nfrom the lack of sufficient training data. Methods: We suggest$\backslash$nand evaluate rotational distortions similar to affine/rotational$\backslash$ndistortions of images to generate augmented data. Results: Our$\backslash$napproach increases the performance of signal processing chains$\backslash$nfor EEG-based brain-computer interfaces when rotating only$\backslash$naround y- and z-axis with an angle around 18 degrees to$\backslash$ngenerate new data. Conclusion: This shows that our processing$\backslash$nefficient approach generates meaningful data and encourages$\backslash$nto look for further new methods for EEG data augmentation.},
author = {Krell, Mario Michael and Kim, Su-Kyoung},
booktitle = {Proceedings of the 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. The 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE EMBC), July 11-15, JeJu Island, South K},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krell, Kim - Unknown - Rotational Data Augmentation for Electroencephalographic Data.pdf:pdf},
number = {Section II},
pages = {n.a.},
title = {{Rotational Data Augmentation for Electroencephalographic Data}},
url = {http://www.dfki.de/web/forschung/publikationen/renameFileForDownload?filename=20170502{\_}Rotational{\%}5CnData{\%}5CnAugmentation{\%}5Cnfor{\%}5CnElectroencephalographic{\%}5CnData.pdf{\&}file{\_}id=uploads{\_}3106},
year = {2017}
}
@article{Bell1995,
abstract = {We d e r i v e a new self-organising learning algorithm which maximises the information transferred in a network of non-linear units. The algo-rithm does not assume any knowledge of the input distributions, and is deened here for the zero-noise limit. Under these conditions, infor-mation maximisation has extra properties not found in the linear case (Linsker 1989). The non-linearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the out-put representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalisation of Principal Components Analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to ten speak-ers. We also show t h a t a v ariant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, w e d e r i v e dependencies of information transfer on time delays. We suggest that information max-imisation provides a unifying framework for problems iblind' signal processing. Please send comments to tony@salk.edu. This paper will appear as Neural Computation, 7, 6, 1004-1034 (1995). The reference for this version is:},
author = {Bell, Anthony J and Sejnowski, Terrence J},
doi = {10.1162/neco.1995.7.6.1129},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bell, Sejnowski - 1995 - An information-maximisation approach t o blind separation and blind deconvolution.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {6},
pages = {1129--1159},
title = {{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.6.1129},
volume = {7},
year = {1995}
}
@inproceedings{Moritz,
abstract = {Teaching machines to read natural language documents remains an elusive chal-lenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {1506.03340},
author = {Hermann, KM and Kocisky, T and Grefenstette, E},
booktitle = {NIPS},
eprint = {1506.03340},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moritz et al. - Unknown - Teaching Machines to Read and Comprehend.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Teaching machines to read and comprehend}},
url = {http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend},
year = {2015}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - Unknown - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Rivet2009,
abstract = {A brain-computer interface (BCI) is a communication system that allows to control a computer or any other device thanks to the brain activity. The BCI described in this paper is based on the P300 speller BCI paradigm introduced by Farwell and Donchin . An unsupervised algorithm is proposed to enhance P300 evoked potentials by estimating spatial filters; the raw EEG signals are then projected into the estimated signal subspace. Data recorded on three subjects were used to evaluate the proposed method. The results, which are presented using a Bayesian linear discriminant analysis classifier , show that the proposed method is efficient and accurate.},
author = {Rivet, Bertrand and Souloumiac, Antoine and Attina, Virginie and Gibert, Guillaume},
doi = {10.1109/TBME.2009.2012869},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivet et al. - 2009 - xDAWN Algorithm to Enhance Evoked Potentials Application to Brain–Computer Interface.pdf:pdf},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Brain-computer interface (BCI),P300 speller,spatial enhancement,xDAWN algorithm},
number = {8},
pages = {2035--2043},
pmid = {19174332},
title = {{xDAWN Algorithm to Enhance Evoked Potentials: Application to Brain-Computer Interface}},
url = {https://journals-scholarsportal-info.myaccess.library.utoronto.ca/pdf/00189294/v56i0008/2035{\_}xateepatbi.xml},
volume = {56},
year = {2009}
}
@article{Zafar2017,
abstract = {Electroencephalogram (EEG)-based decoding human brain activity is challenging, owing to the low spatial resolution of EEG. However, EEG is an important technique, especially for brain–computer interface applications. In this study, a novel algorithm is proposed to decode brain activity associated with different types of images. In this hybrid algorithm, convolu-tional neural network is modified for the extraction of features, a t-test is used for the selec-tion of significant features and likelihood ratio-based score fusion is used for the prediction of brain activity. The proposed algorithm takes input data from multichannel EEG time-series, which is also known as multivariate pattern analysis. Comprehensive analysis was conducted using data from 30 participants. The results from the proposed method are com-pared with current recognized feature extraction and classification/prediction techniques. The wavelet transform-support vector machine method is the most popular currently used feature extraction and prediction method. This method showed an accuracy of 65.7{\%}. How-ever, the proposed method predicts the novel data with improved accuracy of 79.9{\%}. In con-clusion, the proposed algorithm outperformed the current feature extraction and prediction method.},
author = {Zafar, Raheel and Dass, Sarat C and Malik, Aamir Saeed},
doi = {10.1371/journal.pone.0178410},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar, Dass, Saeed Malik - Unknown - Electroencephalogram-based decoding cognitive states using convolutional neural network and likelih.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
title = {{Electroencephalogram-based decoding cognitive states using convolutional neural network and likelihood ratio based score fusion}},
url = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0178410{\&}type=printable},
volume = {12},
year = {2017}
}
@article{Nguyen2012,
annote = {NULL},
author = {Nguyen, Phuoc and Tran, Dat and Huang, Xu and Sharma, Dharmendra},
file = {:home/demetres/Documents/Research Summer 2016/Comparison of Speech and EEG Features.pdf:pdf},
isbn = {1601322178},
journal = {The International Conference on Artificial Intelligence},
keywords = {and fmri,and nirs are expensive,and nirs present long,brain computer interface,eeg,fmri,measure neural activity directly,meg,or bulky,person identification,relying instead on the,they do not,time constants in that},
title = {{A Proposed Feature Extraction Method for EEG-based Person Identification}},
year = {2012}
}
@article{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
doi = {10.18653/v1/D15-1166},
eprint = {1508.04025},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luong, Pham, Manning - 2015 - Effective Approaches to Attention-based Neural Machine Translation.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
keywords = {()},
pmid = {14527267},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {https://arxiv.org/pdf/1508.04025.pdf http://arxiv.org/abs/1508.04025},
year = {2015}
}
@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural$\backslash$nnetwork (RNN) is extended to a bidirectional recurrent neural network$\backslash$n(BRNN). The BRNN can be trained without the limitation of using input$\backslash$ninformation just up to a preset future frame. This is accomplished by$\backslash$ntraining it simultaneously in positive and negative time direction.$\backslash$nStructure and training procedure of the proposed network are explained.$\backslash$nIn regression and classification experiments on artificial data, the$\backslash$nproposed structure gives better results than other approaches. For real$\backslash$ndata, classification experiments for phonemes from the TIMIT database$\backslash$nshow the same tendency. In the second part of this paper, it is shown$\backslash$nhow the proposed bidirectional structure can be easily modified to allow$\backslash$nefficient estimation of the conditional posterior probability of$\backslash$ncomplete symbol sequences without making any explicit assumption about$\backslash$nthe shape of the distribution. For this part, experiments on real data$\backslash$nare reported},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schuster, M. and Paliwal, K.K.},
doi = {10.1109/78.650093},
eprint = {arXiv:1011.1669v3},
file = {:home/demetres/Downloads/00650093.pdf:pdf},
isbn = {1053-587X},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
number = {11},
pages = {2673--2681},
pmid = {25246403},
title = {{Bidirectional recurrent neural networks}},
volume = {45},
year = {1997}
}
@article{Oldfield1971,
author = {Oldfield, R.C.},
doi = {10.1016/0028-3932(71)90067-4},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The Assessment and Analysis of Handedness The Edinburgh Inventory.pdf:pdf},
issn = {00283932},
journal = {Neuropsychologia},
month = {mar},
number = {1},
pages = {97--113},
title = {{The assessment and analysis of handedness: The Edinburgh inventory}},
url = {http://gade.psy.ku.dk/Readings/Oldfield1971.pdf http://linkinghub.elsevier.com/retrieve/pii/0028393271900674},
volume = {9},
year = {1971}
}
@article{Sonderby2015,
abstract = {Machine learning is widely used to analyze biological sequence data. Non-sequential models such as SVMs or feed-forward neural networks are often used although they have no natural way of handling sequences of varying length. Recurrent neural networks such as the long short term memory (LSTM) model on the other hand are designed to handle sequences. In this study we demonstrate that LSTM networks predict the subcellular location of proteins given only the protein sequence with high accuracy (0.902) outperforming current state of the art algorithms. We further improve the performance by introducing convolutional filters and experiment with an attention mechanism which lets the LSTM focus on specific parts of the protein. Lastly we introduce new visualizations of both the convolutional filters and the attention mechanisms and show how they can be used to extract biological relevant knowledge from the LSTM networks.},
archivePrefix = {arXiv},
arxivId = {1503.01919},
author = {S{\o}nderby, S{\o}ren Kaae and S{\o}nderby, Casper Kaae and Nielsen, Henrik and Winther, Ole},
doi = {10.1007/978-3-319-21233-3_6},
eprint = {1503.01919},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\o}nderby et al. - 2015 - Convolutional LSTM Networks for Subcellular Localization of Proteins.pdf:pdf},
isbn = {9783319212326},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional networks,Deep learning,LSTM,Machine learning,Neural networks,RNN,Subcellular location},
month = {mar},
pages = {68--80},
title = {{Convolutional LSTM Networks for Subcellular Localization of Proteins}},
url = {https://arxiv.org/pdf/1503.01919.pdf http://arxiv.org/abs/1503.01919 http://dx.doi.org/10.1007/978-3-319-21233-3{\_}6},
volume = {9199},
year = {2015}
}
@article{Schirrmeister2017,
abstract = {Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, i.e. learning from the raw data. Now, there is increasing interest in using deep ConvNets for end-to-end EEG analysis. However, little is known about many important aspects of how to design and train ConvNets for end-to-end EEG decoding, and there is still a lack of techniques to visualize the informative EEG features the ConvNets learn. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed movements from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching or surpassing that of the widely-used filter bank common spatial patterns (FBCSP) decoding algorithm. While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta and high gamma frequencies. These methods also proved useful as a technique for spatially mapping the learned features, revealing the topography of the causal contributions of features in different frequency bands to decoding the movement classes. Our study thus shows how to design and train ConvNets to decode movement-related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG-based brain mapping.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.05051v1},
author = {Schirrmeister, Robin Tibor and Springenberg, Jost Tobias and Dominique, Lukas and Fiederer, Josef and Glasstetter, Martin and Eggensperger, Katharina and Tangermann, Michael and Hutter, Frank and Burgard, Wolfram and Ball, Tonio},
eprint = {arXiv:1703.05051v1},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schirrmeister et al. - 2017 - Deep learning with convolutional neural networks for brain mapping and decoding of movement-related inform.pdf:pdf},
journal = {arXiv},
keywords = {EEG analysis,Electroencephalography,brain mapping,brain-computer interface (BMI),brain-machine interface (BCI),end-to-end learning,machine learning,model interpretability},
title = {{Deep learning with convolutional neural networks for brain mapping and decoding of movement-related information from the human EEG Short title: Convolutional neural networks in EEG analysis}},
url = {https://arxiv.org/pdf/1703.05051.pdf},
year = {2017}
}
@article{Moreno-Bote2014,
abstract = {Computational strategies used by the brain strongly depend on the amount of information that can be stored in population activity, which in turn strongly depends on the pattern of noise correlations. In vivo, noise correlations tend to be positive and proportional to the similarity in tuning properties. Such correlations are thought to limit information, which has led to the suggestion that decorrelation increases information. In contrast, we found, analytically and numerically, that decorrelation does not imply an increase in information. Instead, the only information-limiting correlations are what we refer to as differential correlations: correlations proportional to the product of the derivatives of the tuning curves. Unfortunately, differential correlations are likely to be very small and buried under correlations that do not limit information, making them particularly difficult to detect. We found, however, that the effect of differential correlations on information can be detected with relatively simple decoders.},
annote = {NULL},
author = {Moreno-Bote, Rub{\'{e}}n and Beck, Jeffrey and Kanitscheider, Ingmar and Pitkow, Xaq and Latham, Peter and Pouget, Alexandre},
doi = {10.1038/nn.3807},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Information-limiting{\_}correlati.PDF.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Brain research,Correlation method (Psychology),Memory storage (Psychology)},
month = {sep},
number = {10},
pages = {1410--1417},
publisher = {Nature Publishing Group},
title = {{Information-limiting correlations}},
url = {http://www.nature.com/doifinder/10.1038/nn.3807},
volume = {17},
year = {2014}
}
@article{KaiKengAng2008,
abstract = {In motor imagery-based brain computer interfaces (BCI), discriminative patterns can be extracted from the electroencephalogram (EEG) using the common spatial pattern (CSP) algorithm. However, the performance of this spatial filter depends on the operational frequency band of the EEG. Thus, setting a broad frequency range, or manually selecting a subject-specific frequency range, are commonly used with the CSP algorithm. To address this problem, this paper proposes a novel filter bank common spatial pattern (FBCSP) to perform autonomous selection of key temporal-spatial discriminative EEG characteristics. After the EEG measurements have been bandpass-filtered into multiple frequency bands, CSP features are extracted from each of these bands. A feature selection algorithm is then used to automatically select discriminative pairs of frequency bands and corresponding CSP features. A classification algorithm is subsequently used to classify the CSP features. A study is conducted to assess the performance of a selection of feature selection and classification algorithms for use with the FBCSP. Extensive experimental results are presented on a publicly available dataset as well as data collected from healthy subjects and unilaterally paralyzed stroke patients. The results show that FBCSP, using a particular combination feature selection and classification algorithm, yields relatively higher cross-validation accuracies compared to prevailing approaches.},
author = {{Kai Keng Ang} and {Zheng Yang Chin} and {Haihong Zhang} and {Cuntai Guan}},
doi = {10.1109/IJCNN.2008.4634130},
file = {:home/demetres/Downloads/04634130.pdf:pdf},
isbn = {978-1-4244-1820-6},
issn = {1098-7576},
journal = {2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},
keywords = {Brain computer interfaces,CSP algorithm,Classification algorithms,Communication system control,EEG,Electroencephalography,Feature extraction,Filter bank,Finite impulse response filter,Frequency measurement,Fuses,Spatial filters,band-pass filters,brain-computer interfaces,classification algorithm,common spatial pattern algorithm,electroencephalogram,electroencephalography,feature extraction,filter bank common spatial pattern,medical signal processing,motor imagery-based brain computer interfaces,spatial filter},
pages = {2390--2397},
title = {{Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4634130},
year = {2008}
}
@article{Muller-Gerking1999,
abstract = {We devised spatial filters for multi-channel EEG that lead to signals which discriminate optimally between two conditions. We demonstrate the effectiveness of this method by classifying single-trial EEGs, recorded during preparation for movements of the left or fight index finger or the fight foot. The classification rates for 3 subjects were 94, 90 and 84{\%}, respectively. The filters are estimated from a set of multi-channel EEG data by the method of Common Spatial Patterns, and reflect the selective activation of cortical areas. By construction, we obtain an automatic weighing of electrodes according to their importance for the classification task. Computationally, this method is parallel by nature, and demands only the evaluation of scalar products. Therefore, it is well suited for on-line data processing. The recognition rates obtained with this relatively simple method are as good as, or higher than those obtained previously with other methods. The high recognition rates and the method's procedural and computational simplicity make it a particularly promising method for an EEG- based brain-computer interface.},
author = {M{\"{u}}ller-Gerking, Johannes and Pfurtscheller, Gert and Flyvbjerg, Henrik},
doi = {10.1016/S1388-2457(98)00038-8},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\`{E}}ller-Gerking, Pfurtscheller, Flyvbjerg - Unknown - Designing optimal spatial {\textregistered}lters for single-trial EEG classi{\textregistered}cation in a movement.pdf:pdf},
isbn = {1388-2457},
issn = {13882457},
journal = {Clinical Neurophysiology},
keywords = {EEG classification,Event-related desynchronization,Human,Mu rhythm,Sensorimotor cortex,Voluntary movement},
number = {5},
pages = {787--798},
pmid = {10400191},
title = {{Designing optimal spatial filters for single-trial EEG classification in a movement task}},
url = {https://journals-scholarsportal-info.myaccess.library.utoronto.ca/pdf/13882457/v110i0005/787{\_}dosffseciamt.xml},
volume = {110},
year = {1999}
}
@article{Ye2017,
abstract = {Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1707.06355},
author = {Ye, Yunan and Zhao, Zhou and Li, Yimeng and Chen, Long and Xiao, Jun and Zhuang, Yueting},
doi = {10.1145/3077136.3080655},
eprint = {1707.06355},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2017 - Video Question Answering via Hierarchical Spatio-Temporal Attention Networks.pdf:pdf},
isbn = {9781450350228},
keywords = {Machine Learning: Data Mining,Machine Learning: Neural Networks,Natural Language Processing: Information Retrieval},
title = {{Video Question Answering via Attribute-Augmented Attention Network Learning}},
url = {https://www.ijcai.org/proceedings/2017/0492.pdf http://arxiv.org/abs/1707.06355{\%}0Ahttp://dx.doi.org/10.1145/3077136.3080655},
year = {2017}
}
@article{Tabar2017,
abstract = {-Adaptation of motor imagery EEG classification model based on tensor decomposition Xinyang Li, Cuntai Guan, Haihong Zhang et al. -Multiresolution analysis over simple graphs for brain computer interfaces J Asensio-Cubero, J Q Gan and R Palaniappan -Recent citations Deep learning with convolutional neural networks for EEG decoding and visualization Robin Tibor Schirrmeister et al -Virtual and Actual Humanoid Robot Control with Four-Class Motor-Imagery-Based Optical Brain-Computer Interface Alyssa M. Batula et al -This content was downloaded from IP address 142.150.190.39 on 15/09/2017 at 16:03},
author = {Tabar, Yousef Rezaei and Halici, Ugur},
doi = {10.1088/1741-2560/14/1/016003},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas, Guan, Lau - Unknown - Adaptive tracking of discriminative frequency components in electroencephalograms for a robust BCI.pdf:pdf},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
month = {feb},
number = {1},
pages = {016003},
title = {{A novel deep learning approach for classification of EEG motor imagery signals}},
url = {http://iopscience.iop.org.myaccess.library.utoronto.ca/article/10.1088/1741-2560/14/1/016003/pdf http://stacks.iop.org/1741-2552/14/i=1/a=016003?key=crossref.2a406e975c74b8476f2c258fdd5ac841},
volume = {14},
year = {2017}
}
@article{Zhu,
abstract = {We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.},
archivePrefix = {arXiv},
arxivId = {1511.03416},
author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
doi = {10.1109/CVPR.2016.540},
eprint = {1511.03416},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - Unknown - Visual7W Grounded Question Answering in Images.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
month = {nov},
title = {{Visual7W: Grounded Question Answering in Images}},
url = {https://arxiv.org/pdf/1409.0473.pdf http://arxiv.org/abs/1409.0473 http://arxiv.org/abs/1511.03416},
year = {2015}
}
@article{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%}). To our knowledge, our result is the first to surpass human-level performance (5.1{\%}, Russakovsky et al.) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
month = {feb},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/He{\_}Delving{\_}Deep{\_}into{\_}ICCV{\_}2015{\_}paper.pdf http://arxiv.org/abs/1502.01852},
year = {2015}
}
@article{Raffel2015,
abstract = {We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic "addition" and "multiplication" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.},
archivePrefix = {arXiv},
arxivId = {1512.08756},
author = {Raffel, Colin and Ellis, Daniel P W},
eprint = {1512.08756},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raffel, Ellis - Unknown - Workshop track -ICLR 2016 FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS.pdf:pdf},
title = {{Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems}},
url = {https://arxiv.org/pdf/1512.08756.pdf http://arxiv.org/abs/1512.08756},
year = {2015}
}
@article{Bahdanau,
abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
archivePrefix = {arXiv},
arxivId = {1508.04395},
author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
eprint = {1508.04395},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau et al. - Unknown - END-TO-END ATTENTION-BASED LARGE VOCABULARY SPEECH RECOGNITION(2).pdf:pdf},
keywords = {ASR,Index Terms— neural networks,LVCSR,attention,speech recognition},
month = {aug},
title = {{End-to-End Attention-based Large Vocabulary Speech Recognition}},
url = {https://arxiv.org/pdf/1508.04395.pdf http://arxiv.org/abs/1508.04395},
year = {2015}
}
@article{Clevert,
abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork-Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
doi = {10.3233/978-1-61499-672-9-1760},
eprint = {1511.07289},
file = {:home/demetres/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clevert, Unterthiner, Hochreiter - Unknown - FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS).pdf:pdf},
isbn = {9781614996712},
issn = {09226389},
title = {{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}},
url = {https://arxiv.org/pdf/1511.07289.pdf http://arxiv.org/abs/1511.07289},
year = {2015}
}
